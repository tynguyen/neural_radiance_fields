{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "full_nerf_example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRkQnr3TrbSf"
      },
      "source": [
        "# How to train your NeRF\n",
        "\n",
        "An example notebook to demonstrate how to train a NeRF model on an example dataset. Unlike the [tiny nerf notebook](https://colab.research.google.com/drive/1rO8xo0TemN67d4mTpakrKrLp03b9bgCX), this uses the 3D coordinates _and the viewing directions_ as input, and _performs hierarchical sampling_ using the scheme suggested in the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib6wg21TrSmW"
      },
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqyJNrRPri_Z"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "We use the [`torchsearchsorted`]() library for the `searchsorted()` module, used in hierarchical sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHqjjGInrg0R"
      },
      "source": [
        "!pip install git+https://github.com/aliutkus/torchsearchsorted\n",
        "import torchsearchsorted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDNfSiwCrn7R"
      },
      "source": [
        "#### Ensure GPU is enabled\n",
        "\n",
        "It'll be very hard to train a NeRF otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6lZ5SP0roQf"
      },
      "source": [
        "assert torch.cuda.is_available()\n",
        "device = \"cuda\"\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66eKf5fxro6m"
      },
      "source": [
        "## Helper functions\n",
        "\n",
        "Basic helpers that help match tensorflow-equivalent functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCzDtEBgrpDM"
      },
      "source": [
        "def meshgrid_xy(tensor1: torch.Tensor, tensor2: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
        "    \"\"\"Mimick np.meshgrid(..., indexing=\"xy\") in pytorch. torch.meshgrid only allows \"ij\" indexing.\n",
        "    (If you're unsure what this means, safely skip trying to understand this, and run a tiny example!)\n",
        "\n",
        "    Args:\n",
        "      tensor1 (torch.Tensor): Tensor whose elements define the first dimension of the returned meshgrid.\n",
        "      tensor2 (torch.Tensor): Tensor whose elements define the second dimension of the returned meshgrid.\n",
        "    \"\"\"\n",
        "    # TESTED\n",
        "    ii, jj = torch.meshgrid(tensor1, tensor2)\n",
        "    return ii.transpose(-1, -2), jj.transpose(-1, -2)\n",
        "\n",
        "\n",
        "def cumprod_exclusive(tensor: torch.Tensor) -> torch.Tensor:\n",
        "    r\"\"\"Mimick functionality of tf.math.cumprod(..., exclusive=True), as it isn't available in PyTorch.\n",
        "\n",
        "    Args:\n",
        "    tensor (torch.Tensor): Tensor whose cumprod (cumulative product, see `torch.cumprod`) along dim=-1\n",
        "      is to be computed.\n",
        "\n",
        "    Returns:\n",
        "    cumprod (torch.Tensor): cumprod of Tensor along dim=-1, mimiciking the functionality of\n",
        "      tf.math.cumprod(..., exclusive=True) (see `tf.math.cumprod` for details).\n",
        "    \"\"\"\n",
        "    # TESTED\n",
        "    # Only works for the last dimension (dim=-1)\n",
        "    dim = -1\n",
        "    # Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
        "    cumprod = torch.cumprod(tensor, dim)\n",
        "    # \"Roll\" the elements along dimension 'dim' by 1 element.\n",
        "    cumprod = torch.roll(cumprod, 1, dim)\n",
        "    # Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
        "    cumprod[..., 0] = 1.\n",
        "\n",
        "    return cumprod"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBqBeeVLrpMA"
      },
      "source": [
        "Ray functions (casting, normalizing, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl9s8lntrpVH"
      },
      "source": [
        "def get_ray_bundle(height: int, width: int, focal_length: float, tform_cam2world: torch.Tensor):\n",
        "    r\"\"\"Compute the bundle of rays passing through all pixels of an image (one ray per pixel).\n",
        "\n",
        "    Args:\n",
        "    height (int): Height of an image (number of pixels).\n",
        "    width (int): Width of an image (number of pixels).\n",
        "    focal_length (float or torch.Tensor): Focal length (number of pixels, i.e., calibrated intrinsics).\n",
        "    tform_cam2world (torch.Tensor): A 6-DoF rigid-body transform (shape: :math:`(4, 4)`) that\n",
        "      transforms a 3D point from the camera frame to the \"world\" frame for the current example.\n",
        "\n",
        "    Returns:\n",
        "    ray_origins (torch.Tensor): A tensor of shape :math:`(width, height, 3)` denoting the centers of\n",
        "      each ray. `ray_origins[i][j]` denotes the origin of the ray passing through pixel at\n",
        "      row index `j` and column index `i`.\n",
        "      (TODO: double check if explanation of row and col indices convention is right).\n",
        "    ray_directions (torch.Tensor): A tensor of shape :math:`(width, height, 3)` denoting the\n",
        "      direction of each ray (a unit vector). `ray_directions[i][j]` denotes the direction of the ray\n",
        "      passing through the pixel at row index `j` and column index `i`.\n",
        "      (TODO: double check if explanation of row and col indices convention is right).\n",
        "    \"\"\"\n",
        "    # TESTED\n",
        "    ii, jj = meshgrid_xy(\n",
        "      torch.arange(width).to(tform_cam2world),\n",
        "      torch.arange(height).to(tform_cam2world)\n",
        "    )\n",
        "    directions = torch.stack([(ii - width * .5) / focal_length,\n",
        "                            -(jj - height * .5) / focal_length,\n",
        "                            -torch.ones_like(ii)\n",
        "                           ], dim=-1)\n",
        "    ray_directions = torch.sum(directions[..., None, :] * tform_cam2world[:3, :3], dim=-1)\n",
        "    ray_origins = tform_cam2world[:3, -1].expand(ray_directions.shape)\n",
        "    return ray_origins, ray_directions\n",
        "\n",
        "\n",
        "def ndc_rays(H, W, focal, near, rays_o, rays_d):\n",
        "    # UNTESTED, but fairly sure.\n",
        "\n",
        "    # Shift rays origins to near plane\n",
        "    t = -(near + rays_o[..., 2]) / rays_d[..., 2]\n",
        "    rays_o = rays_o + t[..., None] * rays_d\n",
        "\n",
        "    # Projection\n",
        "    o0 = -1. / (W / (2. * focal)) * rays_o[..., 0] / rays_o[..., 2]\n",
        "    o1 = -1. / (H / (2. * focal)) * rays_o[..., 1] / rays_o[..., 2]\n",
        "    o2 = 1. + 2. * near / rays_o[..., 2]\n",
        "\n",
        "    d0 = -1. / (W / (2. * focal)) * (rays_d[..., 0] / rays_d[..., 2] - rays_o[..., 0] / rays_o[..., 2] )\n",
        "    d1 = -1. / (H / (2. * focal)) * (rays_d[..., 1] / rays_d[..., 2] - rays_o[..., 1] / rays_o[..., 2] )\n",
        "    d2 = -2. * near / rays_o[..., 2]\n",
        "\n",
        "    rays_o = torch.stack([o0, o1, o2], -1)\n",
        "    rays_d = torch.stack([d0, d1, d2], -1)\n",
        "\n",
        "    return rays_o, rays_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeUEDnjorpd6"
      },
      "source": [
        "## Differentiable volume rendering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DY2Xb7IrppE"
      },
      "source": [
        "def gather_cdf_util(cdf, inds):\n",
        "    r\"\"\"A very contrived way of mimicking a version of the tf.gather()\n",
        "    call used in the original impl.\n",
        "    \"\"\"\n",
        "    orig_inds_shape = inds.shape\n",
        "    inds_flat = [inds[i].view(-1) for i in range(inds.shape[0])]\n",
        "    valid_mask = [torch.where(ind >= cdf.shape[1], torch.zeros_like(ind), torch.ones_like(ind)) for ind in inds_flat]\n",
        "    inds_flat = [torch.where(ind >= cdf.shape[1], (cdf.shape[1] - 1) * torch.ones_like(ind), ind) for ind in inds_flat]\n",
        "    cdf_flat = [cdf[i][ind] for i, ind in enumerate(inds_flat)]\n",
        "    cdf_flat = [cdf_flat[i] * valid_mask[i] for i in range(len(cdf_flat))]\n",
        "    cdf_flat = [cdf_chunk.reshape([1] + list(orig_inds_shape[1:])) for cdf_chunk in cdf_flat]\n",
        "    return torch.cat(cdf_flat, dim=0)\n",
        "\n",
        "\n",
        "def sample_pdf(bins, weights, num_samples, det=False):\n",
        "    # TESTED (Carefully, line-to-line).\n",
        "    # But chances of bugs persist; haven't integration-tested with\n",
        "    # training routines.\n",
        "\n",
        "    # Get pdf\n",
        "    weights = weights + 1e-5  # prevent nans\n",
        "    pdf = weights / weights.sum(-1).unsqueeze(-1)\n",
        "    cdf = torch.cumsum(pdf, -1)\n",
        "    cdf = torch.cat((torch.zeros_like(cdf[..., :1]), cdf), -1)\n",
        "\n",
        "    # Take uniform samples\n",
        "    if det:\n",
        "        u = torch.linspace(0., 1., num_samples).to(weights)\n",
        "        u = u.expand(list(cdf.shape[:-1]) + [num_samples])\n",
        "    else:\n",
        "        u = torch.rand(list(cdf.shape[:-1]) + [num_samples]).to(weights)\n",
        "\n",
        "    # Invert CDF\n",
        "    inds = torchsearchsorted.searchsorted(cdf.contiguous(), u.contiguous(), side='right')\n",
        "    below = torch.max(torch.zeros_like(inds), inds-1)\n",
        "    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n",
        "    inds_g = torch.stack((below, above), -1)\n",
        "    orig_inds_shape = inds_g.shape\n",
        "\n",
        "    cdf_g = gather_cdf_util(cdf, inds_g)\n",
        "    bins_g = gather_cdf_util(bins, inds_g)\n",
        "\n",
        "    denom = cdf_g[..., 1] - cdf_g[..., 0]\n",
        "    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
        "    t = (u - cdf_g[..., 0]) / denom\n",
        "    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
        "\n",
        "    return samples\n",
        "\n",
        "\n",
        "def volume_render_radiance_field(radiance_field, depth_values,\n",
        "                                 ray_directions,\n",
        "                                 radiance_field_noise_std = 0.,\n",
        "                                 white_background = False):\n",
        "    # TESTED\n",
        "    one_e_10 = torch.tensor([1e10], dtype=ray_directions.dtype,\n",
        "                            device=ray_directions.device)\n",
        "    dists = torch.cat((depth_values[..., 1:] - depth_values[..., :-1],\n",
        "                       one_e_10.expand(depth_values[..., :1].shape)), dim=-1)\n",
        "    dists = dists * ray_directions[..., None, :].norm(p=2, dim=-1)\n",
        "\n",
        "    rgb = torch.sigmoid(radiance_field[..., :3])\n",
        "    noise = 0.\n",
        "    if radiance_field_noise_std > 0.:\n",
        "        noise = torch.randn(radiance_field[..., 3].shape) * radiance_field_noise_std\n",
        "        noise = noise.to(radiance_field)\n",
        "    sigma_a = torch.nn.functional.relu(radiance_field[..., 3] + noise)\n",
        "    alpha = 1. - torch.exp(-sigma_a * dists)\n",
        "    weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
        "\n",
        "    rgb_map = weights[..., None] * rgb\n",
        "    rgb_map = rgb_map.sum(dim=-2)\n",
        "    depth_map = weights * depth_values\n",
        "    depth_map = depth_map.sum(dim=-1)\n",
        "    # depth_map = (weights * depth_values).sum(dim=-1)\n",
        "    acc_map = weights.sum(dim=-1)\n",
        "    disp_map = 1. / torch.max(\n",
        "        1e-10 * torch.ones_like(depth_map), depth_map / acc_map\n",
        "    )\n",
        "\n",
        "    if white_background:\n",
        "        rgb_map = rgb_map + (1. - acc_map[..., None])\n",
        "\n",
        "    return rgb_map, disp_map, acc_map, weights, depth_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewrUXSxCr9Lc"
      },
      "source": [
        "## Train/Eval routines\n",
        "\n",
        "Functions that implement the train/eval loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r14nEYIhr9TE"
      },
      "source": [
        "def get_minibatches(inputs: torch.Tensor, chunksize: int = 1024 * 8):\n",
        "    r\"\"\"Takes a huge tensor (ray \"bundle\") and splits it into a list of minibatches.\n",
        "    Each element of the list (except possibly the last) has dimension `0` of length\n",
        "    `chunksize`.\n",
        "    \"\"\"\n",
        "    return [inputs[i:i + chunksize] for i in range(0, inputs.shape[0], chunksize)]\n",
        "\n",
        "\n",
        "def run_network(network_fn, pts, ray_batch, chunksize, embed_fn,\n",
        "                embeddirs_fn):\n",
        "\n",
        "    pts_flat = pts.reshape((-1, pts.shape[-1]))\n",
        "    embedded = embed_fn(pts_flat)\n",
        "    if embeddirs_fn is not None:\n",
        "        viewdirs = ray_batch[..., None, -3:]\n",
        "        input_dirs = viewdirs.expand(pts.shape)\n",
        "        input_dirs_flat = input_dirs.reshape((-1, input_dirs.shape[-1]))\n",
        "        embedded_dirs = embeddirs_fn(input_dirs_flat)\n",
        "        embedded = torch.cat((embedded, embedded_dirs), dim=-1)\n",
        "\n",
        "    batches = get_minibatches(embedded, chunksize=chunksize)\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        preds.append(network_fn(batch))\n",
        "    radiance_field = torch.cat(preds, dim=0)\n",
        "    radiance_field = radiance_field.reshape(list(pts.shape[:-1]) + [radiance_field.shape[-1]])\n",
        "    return radiance_field\n",
        "\n",
        "\n",
        "def positional_encoding(\n",
        "    tensor, num_encoding_functions=6, include_input=True\n",
        ") -> torch.Tensor:\n",
        "    r\"\"\"Apply positional encoding to the input.\n",
        "\n",
        "    Args:\n",
        "    tensor (torch.Tensor): Input tensor to be positionally encoded.\n",
        "    encoding_size (optional, int): Number of encoding functions used to compute\n",
        "        a positional encoding (default: 6).\n",
        "    include_input (optional, bool): Whether or not to include the input in the\n",
        "        positional encoding (default: True)\n",
        "\n",
        "    Returns:\n",
        "    (torch.Tensor): Positional encoding of the input tensor.\n",
        "    \"\"\"\n",
        "    # TESTED\n",
        "    # Trivially, the input tensor is added to the positional encoding.\n",
        "    encoding = [tensor] if include_input else []\n",
        "    # Now, encode the input using a set of high-frequency functions and append the\n",
        "    # resulting values to the encoding.\n",
        "    for i in range(num_encoding_functions):\n",
        "        for func in [torch.sin, torch.cos]:\n",
        "          encoding.append(func(2. ** i * tensor))\n",
        "    return torch.cat(encoding, dim=-1)\n",
        "\n",
        "\n",
        "def img2mse(img_src, img_tgt):\n",
        "    return torch.nn.functional.mse_loss(img_src, img_tgt)\n",
        "\n",
        "\n",
        "def mse2psnr(mse):\n",
        "    return -10. * math.log10(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P0RO7Wbsf9b"
      },
      "source": [
        "## NeRF forward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAyLKW7BsaPC"
      },
      "source": [
        "def predict_and_render_radiance(\n",
        "    ray_batch, model_coarse, model_fine, num_coarse, num_fine, chunksize,\n",
        "    mode=\"train\", lindisp=False, perturb=True,\n",
        "    encode_position_fn=positional_encoding,\n",
        "    encode_direction_fn=positional_encoding,\n",
        "    radiance_field_noise_std=0.,\n",
        "    white_background=False\n",
        "):\n",
        "    # TESTED\n",
        "    num_rays = ray_batch.shape[0]\n",
        "    ro, rd = ray_batch[..., :3], ray_batch[..., 3:6]\n",
        "    bounds = ray_batch[..., 6:8].reshape((-1, 1, 2))\n",
        "    near, far = bounds[..., 0], bounds[..., 1]\n",
        "\n",
        "    # TODO: Use actual values for \"near\" and \"far\" (instead of 0. and 1.)\n",
        "    # when not enabling \"ndc\".\n",
        "    t_vals = torch.linspace(0., 1., num_coarse).to(ro)\n",
        "    if not lindisp:\n",
        "        z_vals = near * (1. - t_vals) + far * t_vals\n",
        "    else:\n",
        "        z_vals = 1. / (1. / near * (1. - t_vals) + 1. / far * t_vals)\n",
        "    z_vals = z_vals.expand([num_rays, num_coarse])\n",
        "\n",
        "    if perturb:\n",
        "        # Get intervals between samples.\n",
        "        mids = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
        "        upper = torch.cat((mids, z_vals[..., -1:]), dim=-1)\n",
        "        lower = torch.cat((z_vals[..., :1], mids), dim=-1)\n",
        "        # Stratified samples in those intervals.\n",
        "        t_rand = torch.rand(z_vals.shape).to(ro)\n",
        "        z_vals = lower + (upper - lower) * t_rand\n",
        "    # pts -> (num_rays, N_samples, 3)\n",
        "    pts = ro[..., None, :] + rd[..., None, :] * z_vals[..., :, None]\n",
        "\n",
        "    # encode_position_fn = None\n",
        "    # encode_direction_fn = None\n",
        "    # if encode_position_fn == \"positional_encoding\":\n",
        "    #     encode_position_fn = positional_encoding\n",
        "    # if encode_direction_fn == \"positional_encoding\":\n",
        "    #     encode_direction_fn = positional_encoding\n",
        "\n",
        "    radiance_field = run_network(model_coarse,\n",
        "                                 pts,\n",
        "                                 ray_batch,\n",
        "                                 chunksize,\n",
        "                                 encode_position_fn,\n",
        "                                 encode_direction_fn,\n",
        "                                )\n",
        "    \n",
        "    rgb_coarse, disp_coarse, acc_coarse, weights, depth_coarse = volume_render_radiance_field(\n",
        "        radiance_field, z_vals, rd,\n",
        "        radiance_field_noise_std=radiance_field_noise_std,\n",
        "        white_background=white_background\n",
        "    )\n",
        "\n",
        "    # TODO: Implement importance sampling, and finer network.\n",
        "    rgb_fine, disp_fine, acc_fine = None, None, None\n",
        "    if num_fine > 0:\n",
        "        # rgb_map_0, disp_map_0, acc_map_0 = rgb_map, disp_map, acc_map\n",
        "\n",
        "        z_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
        "        z_samples = sample_pdf(\n",
        "            z_vals_mid, weights[..., 1:-1], num_fine,\n",
        "            det=(perturb == 0.)\n",
        "        )\n",
        "        z_samples = z_samples.detach()\n",
        "\n",
        "        z_vals, _ = torch.sort(torch.cat((z_vals, z_samples), dim=-1), dim=-1)\n",
        "        # pts -> (N_rays, N_samples + N_importance, 3)\n",
        "        pts = ro[..., None, :] + rd[..., None, :] * z_vals[..., :, None]\n",
        "\n",
        "        radiance_field = run_network(model_fine,\n",
        "                                     pts,\n",
        "                                     ray_batch,\n",
        "                                     chunksize,\n",
        "                                     encode_position_fn,\n",
        "                                     encode_direction_fn\n",
        "                                    )\n",
        "        rgb_fine, disp_fine, acc_fine, _, _ = volume_render_radiance_field(\n",
        "            radiance_field, z_vals, rd,\n",
        "            radiance_field_noise_std=radiance_field_noise_std,\n",
        "            white_background=white_background\n",
        "        )\n",
        "\n",
        "    return rgb_coarse, disp_coarse, acc_coarse, rgb_fine, disp_fine, acc_fine\n",
        "\n",
        "\n",
        "def nerf_forward_pass(\n",
        "    H, W, focal, model_coarse, model_fine, batch_rays,\n",
        "    use_viewdirs, near, far, chunksize, num_coarse, num_fine, mode=\"train\",\n",
        "    lindisp=False, perturb=True, encode_position_fn=positional_encoding,\n",
        "    encode_direction_fn=positional_encoding, radiance_field_noise_std=0.,\n",
        "    white_background=False\n",
        "):\n",
        "    ray_origins = batch_rays[0]\n",
        "    ray_directions = batch_rays[1]\n",
        "    if use_viewdirs:\n",
        "        # Provide ray directions as input\n",
        "        viewdirs = ray_directions\n",
        "        viewdirs = viewdirs / viewdirs.norm(p=2, dim=-1).unsqueeze(-1)\n",
        "        viewdirs = viewdirs.reshape((-1, 3))\n",
        "    ray_shapes = ray_directions.shape  # Cache now, to restore later.\n",
        "    ro, rd = ndc_rays(H, W, focal, 1., ray_origins, ray_directions)\n",
        "    ro = ro.reshape((-1, 3))\n",
        "    rd = rd.reshape((-1, 3))\n",
        "    near = near * torch.ones_like(rd[..., :1])\n",
        "    far = far * torch.ones_like(rd[..., :1])\n",
        "    rays = torch.cat((ro, rd, near, far), dim=-1)\n",
        "    if use_viewdirs:\n",
        "        rays = torch.cat((rays, viewdirs), dim=-1)\n",
        "\n",
        "    batches = get_minibatches(rays, chunksize=chunksize)\n",
        "    # TODO: Init a list, keep appending outputs to that list,\n",
        "    # concat everything in the end.\n",
        "    rgb_coarse, disp_coarse, acc_coarse = [], [], []\n",
        "    rgb_fine, disp_fine, acc_fine = None, None, None\n",
        "    for batch in batches:\n",
        "        rc, dc, ac, rf, df, af = predict_and_render_radiance(\n",
        "            batch, model_coarse, model_fine, num_coarse, num_fine, chunksize,\n",
        "            mode, lindisp=lindisp, perturb=perturb,\n",
        "            encode_position_fn=encode_position_fn,\n",
        "            encode_direction_fn=encode_direction_fn,\n",
        "            radiance_field_noise_std=radiance_field_noise_std,\n",
        "            white_background=white_background\n",
        "        )\n",
        "        rgb_coarse.append(rc)\n",
        "        disp_coarse.append(dc)\n",
        "        acc_coarse.append(ac)\n",
        "        if rf is not None:\n",
        "            if rgb_fine is None:\n",
        "                rgb_fine = [rf]\n",
        "            else:\n",
        "                rgb_fine.append(rf)\n",
        "        if df is not None:\n",
        "            if disp_fine is None:\n",
        "                disp_fine = [df]\n",
        "            else:\n",
        "                disp_fine.append(df)\n",
        "        if af is not None:\n",
        "            if acc_fine is None:\n",
        "                acc_fine = [af]\n",
        "            else:\n",
        "                acc_fine.append(af)\n",
        "\n",
        "    rgb_coarse_ = torch.cat(rgb_coarse, dim=0)\n",
        "    disp_coarse_ = torch.cat(disp_coarse, dim=0)\n",
        "    acc_coarse_ = torch.cat(acc_coarse, dim=0)\n",
        "    if rgb_fine is not None:\n",
        "        rgb_fine_ = torch.cat(rgb_fine, dim=0)\n",
        "    else:\n",
        "        rgb_fine_ = None\n",
        "    if disp_fine is not None:\n",
        "        disp_fine_ = torch.cat(disp_fine, dim=0)\n",
        "    else:\n",
        "        disp_fine_ = None\n",
        "    if acc_fine is not None:\n",
        "        acc_fine_ = torch.cat(acc_fine, dim=0)\n",
        "    else:\n",
        "        acc_fine_ = None\n",
        "\n",
        "    return rgb_coarse_, disp_coarse_, acc_coarse_, rgb_fine_, disp_fine_, acc_fine_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53oJFIXCr9Z4"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "For convenience, we only show a low-res example in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOk79AHAr9kc"
      },
      "source": [
        "# Download and extract data\n",
        "!mkdir data\n",
        "!wget -P data https://people.eecs.berkeley.edu/~bmild/nerf/nerf_example_data.zip\n",
        "!unzip data/nerf_example_data.zip -d data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2f4HDi5s2T_"
      },
      "source": [
        "Utils to load NeRF data into PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN6y5GxWs0dK"
      },
      "source": [
        "def translate_by_t_along_z(t):\n",
        "    tform = np.eye(4).astype(np.float32)\n",
        "    tform[2][3] = t\n",
        "    return tform\n",
        "\n",
        "\n",
        "def rotate_by_phi_along_x(phi):\n",
        "    tform = np.eye(4).astype(np.float32)\n",
        "    tform[1, 1] = tform[2, 2] = np.cos(phi)\n",
        "    tform[1, 2] = -np.sin(phi)\n",
        "    tform[2, 1] = -tform[1, 2]\n",
        "    return tform\n",
        "\n",
        "def rotate_by_theta_along_y(theta):\n",
        "    tform = np.eye(4).astype(np.float32)\n",
        "    tform[0, 0] = tform[2, 2] = np.cos(theta)\n",
        "    tform[0, 2] = -np.sin(theta)\n",
        "    tform[2, 0] = -tform[0, 2]\n",
        "    return tform\n",
        "\n",
        "\n",
        "def pose_spherical(theta, phi, radius):\n",
        "    c2w = translate_by_t_along_z(radius)\n",
        "    c2w = rotate_by_phi_along_x(phi / 180. * np.pi) @ c2w\n",
        "    c2w = rotate_by_theta_along_y(theta / 180 * np.pi) @ c2w\n",
        "    c2w = np.array([\n",
        "        [-1, 0, 0, 0],\n",
        "        [0, 0, 1, 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [0, 0, 0, 1]\n",
        "    ]) @ c2w\n",
        "    return c2w\n",
        "\n",
        "\n",
        "def load_blender_data(basedir, downscaleFactor=8, testskip=1):\n",
        "    splits = [\"train\", \"val\", \"test\"]\n",
        "    metas = {}\n",
        "    for s in splits:\n",
        "        with open(os.path.join(basedir, f\"transforms_{s}.json\"), \"r\") as fp:\n",
        "            metas[s] = json.load(fp)\n",
        "\n",
        "    all_imgs = []\n",
        "    all_poses = []\n",
        "    counts = [0]\n",
        "    for s in splits:\n",
        "        meta = metas[s]\n",
        "        imgs = []\n",
        "        poses = []\n",
        "        if s == \"train\" or testskip == 0:\n",
        "            skip = 1\n",
        "        else:\n",
        "            skip = testskip\n",
        "\n",
        "        for frame in meta[\"frames\"][::skip]:\n",
        "            fname = os.path.join(basedir, frame[\"file_path\"] + \".png\")\n",
        "            imgs.append(imageio.imread(fname))\n",
        "            poses.append(np.array(frame[\"transform_matrix\"]))\n",
        "        imgs = (np.array(imgs) / 255.).astype(np.float32)\n",
        "        poses = np.array(poses).astype(np.float32)\n",
        "        counts.append(counts[-1] + imgs.shape[0])\n",
        "        all_imgs.append(imgs)\n",
        "        all_poses.append(poses)\n",
        "\n",
        "    i_split = [np.arange(counts[i], counts[i + 1]) for i in range(3)]\n",
        "\n",
        "    imgs = np.concatenate(all_imgs, 0)\n",
        "    poses = np.concatenate(all_poses, 0)\n",
        "\n",
        "    H, W = imgs[0].shape[:2]\n",
        "    camera_angle_x = float(meta[\"camera_angle_x\"])\n",
        "    focal = .5 * W / np.tan(.5 * camera_angle_x)\n",
        "\n",
        "    render_poses = torch.stack([\n",
        "        torch.from_numpy(\n",
        "            pose_spherical(angle, -30., 4.)\n",
        "        ) for angle in np.linspace(-180, 180, 40 + 1)[:-1]\n",
        "    ], 0)\n",
        "\n",
        "    # Apply scale factor\n",
        "    targetsize = imgs.shape[1] // downscaleFactor\n",
        "    H = H // downscaleFactor\n",
        "    W = W // downscaleFactor\n",
        "    focal = focal / downscaleFactor\n",
        "    imgs = [torch.from_numpy(\n",
        "        cv2.resize(imgs[i], dsize=(targetsize, targetsize), interpolation=cv2.INTER_AREA)\n",
        "    ) for i in range(imgs.shape[0])]\n",
        "    imgs = torch.stack(imgs, 0)\n",
        "\n",
        "    poses = torch.from_numpy(poses)\n",
        "\n",
        "    return imgs, poses, render_poses, [H, W, focal], i_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwQIFIw_s-xO"
      },
      "source": [
        "# Load data\n",
        "images, poses, render_poses, hwf, i_split = load_blender_data(\n",
        "    \"data/nerf_synthetic/lego\",\n",
        "    downscaleFactor=16,\n",
        "    testskip=1\n",
        ")\n",
        "near = 0.\n",
        "far = 1.\n",
        "\n",
        "i_train, i_val, i_test = i_split\n",
        "H, W, focal = hwf\n",
        "H, W = int(H), int(W)\n",
        "hwf = [H, W, focal]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egdi06der9tw"
      },
      "source": [
        "Visualize a few samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOWnHeXqr91q"
      },
      "source": [
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "fig = plt.figure(figsize=(16, 16))\n",
        "imagegrid = ImageGrid(fig, 111, nrows_ncols=(4, 4))\n",
        "imagesamples = [images[i] for i in range(0, 80, 5)]\n",
        "for ax, im in zip(imagegrid, imagesamples):\n",
        "    ax.imshow(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5hF4XRir98C"
      },
      "source": [
        "## Define the NeRF model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03u7zHa-r-DC"
      },
      "source": [
        "class VeryTinyNeRFModel(torch.nn.Module):\n",
        "    r\"\"\"Define a \"very tiny\" NeRF model comprising three fully connected layers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, filter_size=128, num_encoding_fn_xyz=6, num_encoding_fn_dir=4,\n",
        "        use_viewdirs=True, include_input_xyz=True, include_input_dir=True\n",
        "    ):\n",
        "        super(VeryTinyNeRFModel, self).__init__()\n",
        "\n",
        "        self.include_input_xyz = 3 if include_input_xyz else 0\n",
        "        self.include_input_dir = 3 if include_input_dir else 0\n",
        "        self.dim_xyz = self.include_input_xyz + 2 * 3 * num_encoding_fn_xyz\n",
        "        self.dim_dir = self.include_input_dir + 2 * 3 * num_encoding_fn_dir\n",
        "        if not use_viewdirs:\n",
        "          self.dim_dir = 0\n",
        "\n",
        "        # Input layer (default: 65 -> 128)\n",
        "        self.layer1 = torch.nn.Linear(self.dim_xyz + self.dim_dir, filter_size)\n",
        "        # Layer 2 (default: 128 -> 128)\n",
        "        self.layer2 = torch.nn.Linear(filter_size, filter_size)\n",
        "        # Layer 3 (default: 128 -> 4)\n",
        "        self.layer3 = torch.nn.Linear(filter_size, 4)\n",
        "        # Short hand for torch.nn.functional.relu\n",
        "        self.relu = torch.nn.functional.relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ReplicateNeRFModel(torch.nn.Module):\n",
        "    r\"\"\"NeRF model that (almost) follows the figure (from the supp. material of \n",
        "    the NeRF paper), but has about half the number of layers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, hidden_size=128, num_encoding_fn_xyz=6, num_encoding_fn_dir=4,\n",
        "        include_input_xyz=True, include_input_dir=True\n",
        "    ):\n",
        "        super(ReplicateNeRFModel, self).__init__()\n",
        "        \n",
        "        self.include_input_xyz = 3 if include_input_xyz else 0\n",
        "        self.include_input_dir = 3 if include_input_dir else 0\n",
        "        self.dim_xyz = self.include_input_xyz + 2 * 3 * num_encoding_fn_xyz\n",
        "        self.dim_dir = self.include_input_dir + 2 * 3 * num_encoding_fn_dir\n",
        "\n",
        "        self.layer1 = torch.nn.Linear(self.dim_xyz, hidden_size)\n",
        "        self.layer2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = torch.nn.Linear(hidden_size, 1 + hidden_size)\n",
        "        self.layer4 = torch.nn.Linear(hidden_size + self.dim_dir, hidden_size // 2)\n",
        "        self.layer5 = torch.nn.Linear(hidden_size // 2, hidden_size // 2)\n",
        "        self.layer6 = torch.nn.Linear(hidden_size // 2, 3)\n",
        "        self.relu = torch.nn.functional.relu\n",
        "    \n",
        "    def forward(self, x):\n",
        "        xyz = x[..., :self.dim_xyz]\n",
        "        direction = x[..., self.dim_xyz:]\n",
        "        x_ = self.relu(self.layer1(xyz))\n",
        "        x_ = self.relu(self.layer2(x_))\n",
        "        sigma_feat = self.layer3(x_)\n",
        "        sigma = sigma_feat[..., 0].unsqueeze(-1)\n",
        "        feat = sigma_feat[..., 1:]\n",
        "        y_ = self.relu(self.layer4(torch.cat((feat, direction), dim=-1)))\n",
        "        y_ = self.relu(self.layer5(y_))\n",
        "        y_ = self.layer6(y_)\n",
        "        return torch.cat((y_, sigma), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xuwehmds76n"
      },
      "source": [
        "## Run training / validation\n",
        "\n",
        "This should take a WHILE (3-4 hours or more) to converge. But, you should be able to see training progress soon enough (in the first 1000 iterations)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31sNNVves8C2"
      },
      "source": [
        "# seed = 1234\n",
        "seed = 2350\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "######################################\n",
        "# CONFIGURE EXPERIMENT PARAMETERS\n",
        "######################################\n",
        "# Whether to use the fine-level model\n",
        "use_fine_model = True\n",
        "# Whether to use viewing directions as additional inputs\n",
        "use_viewdirs = True\n",
        "# Number of rays to sample per minibatch\n",
        "num_random_rays = 512\n",
        "# Number of coarse depths to sample\n",
        "num_coarse = 32\n",
        "# Number of fine depths to sample\n",
        "num_fine = 32\n",
        "# Chunksize (minibatch to pass to the neural net at one time)\n",
        "chunksize = 4096\n",
        "# Whether to include the position (xyz) itself in its positional encoding\n",
        "include_input_xyz = False\n",
        "# Number of encoding functions to use for positional encoding of the coordinates\n",
        "num_encoding_fn_xyz = 6\n",
        "# Whether to include the direction itself in its positional encoding\n",
        "include_input_dir = False\n",
        "# Number of encoding functions to use for positional encoding of the direction\n",
        "num_encoding_fn_dir = 6\n",
        "# Function used to encode position\n",
        "encode_position_fn = lambda x: positional_encoding(\n",
        "    x, num_encoding_functions=num_encoding_fn_xyz, include_input=include_input_xyz\n",
        ")\n",
        "# Function used to encode direction\n",
        "encode_direction_fn = lambda x: positional_encoding(\n",
        "    x, num_encoding_functions=num_encoding_fn_dir, include_input=include_input_dir\n",
        ")\n",
        "# Number of iterations to run optimization for\n",
        "num_iters = 5000\n",
        "# Number of iterations after which to validate once\n",
        "validate_every = 50\n",
        "# Number of iterations after which to save model\n",
        "save_every = 1000\n",
        "# Learning rate for optimizer\n",
        "learning_rate = 5e-3\n",
        "######################################\n",
        "\n",
        "# Initialize a coarse-resolution model.\n",
        "# model_coarse = ReplicateNeRFModel(\n",
        "#     hidden_size=128,\n",
        "#     num_encoding_fn_xyz=num_encoding_fn_xyz,\n",
        "#     num_encoding_fn_dir=num_encoding_fn_dir,\n",
        "#     include_input_xyz=include_input_xyz,\n",
        "#     include_input_dir=include_input_dir\n",
        "# )\n",
        "model_coarse = VeryTinyNeRFModel()\n",
        "model_coarse.to(device)\n",
        "\n",
        "# Initialize a fine-resolution model, if specified.\n",
        "# model_fine = ReplicateNeRFModel(\n",
        "#     hidden_size=128,\n",
        "#     num_encoding_fn_xyz=num_encoding_fn_xyz,\n",
        "#     num_encoding_fn_dir=num_encoding_fn_dir,\n",
        "#     include_input_xyz=include_input_xyz,\n",
        "#     include_input_dir=include_input_dir\n",
        "# )\n",
        "model_fine = VeryTinyNeRFModel()\n",
        "model_fine.to(device)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model_coarse.parameters()) + list(model_fine.parameters()),\n",
        "    lr=learning_rate\n",
        ")\n",
        "\n",
        "i = 0\n",
        "\n",
        "train_losses = []\n",
        "train_iternums = []\n",
        "val_losses = []\n",
        "val_iternums = []\n",
        "\n",
        "# Train/Eval loop\n",
        "for i in range(i, num_iters):\n",
        "\n",
        "    if i == 64000:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = 5e-3\n",
        "        print(\"Updated learning rate!\")\n",
        "\n",
        "    # Train\n",
        "    img_idx = np.random.choice(i_train)\n",
        "    img_target = images[img_idx].to(device)\n",
        "    pose_target = poses[img_idx, :3, :4].to(device)\n",
        "    ray_origins, ray_directions = get_ray_bundle(H, W, focal, pose_target)\n",
        "    coords = torch.stack(meshgrid_xy(torch.arange(H).to(device),\n",
        "                                      torch.arange(W).to(device)), dim=-1)\n",
        "    coords = coords.reshape((-1, 2))\n",
        "    # select_inds = np.random.choice(\n",
        "    #     coords.shape[0], size=(num_random_rays), replace=False\n",
        "    # )\n",
        "    # select_inds = coords[select_inds]\n",
        "    # ray_origins = ray_origins[select_inds[:, 0], select_inds[:, 1], :]\n",
        "    # ray_directions = ray_directions[select_inds[:, 0], select_inds[:, 1], :]\n",
        "    batch_rays = torch.stack([ray_origins, ray_directions], dim=0)\n",
        "    # target_ray_values = img_target[select_inds[:, 0], select_inds[:, 1], :]\n",
        "    target_ray_values = img_target.reshape((-1, 4))\n",
        "    \n",
        "    rgb_coarse, _, _, rgb_fine, _, _ = nerf_forward_pass(\n",
        "        H, W, focal, model_coarse, model_fine, batch_rays, use_viewdirs=use_viewdirs,\n",
        "        near=near, far=far, chunksize=chunksize, num_coarse=num_coarse,\n",
        "        num_fine=num_fine, mode=\"train\", lindisp=False, perturb=True,\n",
        "        encode_position_fn=encode_position_fn,\n",
        "        encode_direction_fn=encode_direction_fn\n",
        "    )\n",
        "\n",
        "    coarse_loss = img2mse(rgb_coarse[..., :3], target_ray_values[..., :3])\n",
        "    fine_loss = img2mse(rgb_fine[..., :3], target_ray_values[..., :3])\n",
        "\n",
        "    loss = coarse_loss + fine_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "    train_iternums.append(i)\n",
        "\n",
        "\n",
        "    # Validate\n",
        "    if i % validate_every == 0:\n",
        "        print(\"Train loss:\", loss.item(), \"Train PSNR:\", mse2psnr(loss.item()))\n",
        "        img_idx = np.random.choice(i_val)\n",
        "        img_target = images[img_idx].to(device)\n",
        "        pose_target = poses[img_idx, :3, :4].to(device)\n",
        "        ray_origins, ray_directions = get_ray_bundle(H, W, focal, pose_target)\n",
        "        original_shape = ray_origins.shape\n",
        "        batch_rays = torch.stack([ray_origins, ray_directions], dim=0)\n",
        "        rgb_coarse, _, _, rgb_fine, _, _ = nerf_forward_pass(\n",
        "            H, W, focal, model_coarse, model_fine, batch_rays, use_viewdirs=use_viewdirs,\n",
        "            near=near, far=far, chunksize=chunksize, num_coarse=num_coarse,\n",
        "            num_fine=num_fine, mode=\"validation\", lindisp=False, perturb=False,\n",
        "            encode_position_fn=encode_position_fn, encode_direction_fn=encode_direction_fn\n",
        "        )\n",
        "        rgb_coarse = rgb_coarse.reshape(original_shape)\n",
        "        rgb_fine = rgb_fine.reshape(original_shape)\n",
        "        coarse_loss = img2mse(rgb_coarse[..., :3], img_target[..., :3])\n",
        "        fine_loss = img2mse(rgb_coarse[..., :3], img_target[..., :3])\n",
        "        loss = coarse_loss + fine_loss\n",
        "        print(\"Val loss:\", loss.item(), \"Val PSNR:\", mse2psnr(loss.item()))\n",
        "        val_losses.append(loss.item())\n",
        "        val_iternums.append(i)\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.subplot(131)\n",
        "        plt.imshow(img_target[..., :3].detach().cpu().numpy())\n",
        "        plt.title(f\"Iteration {i}: Ground-truth image\")\n",
        "        plt.subplot(132)\n",
        "        plt.imshow(rgb_coarse[..., :3].detach().cpu().numpy())\n",
        "        plt.title(\"Coarse image\")\n",
        "        plt.subplot(133)\n",
        "        plt.imshow(rgb_fine[..., :3].detach().cpu().numpy())\n",
        "        plt.title(\"Fine image\")\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFsj81RFs8JS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO2zCJTfs8Yp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ht7PmbQs88r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}